<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>conbrad.ca</title>
<link>https://conbrad.ca/index.html</link>
<atom:link href="https://conbrad.ca/index.xml" rel="self" type="application/rss+xml"/>
<description>A collection of technical blogs and talks on machine learning and data science.</description>
<image>
<url>https://conbrad.ca/quarto.png</url>
<title>conbrad.ca</title>
<link>https://conbrad.ca/index.html</link>
</image>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Sat, 06 Apr 2024 07:00:00 GMT</lastBuildDate>
<item>
  <title>Unexpected {Simplicity, Complexity}</title>
  <dc:creator>Conor Brady</dc:creator>
  <link>https://conbrad.ca/notes/backprop-recursive-chain-rule/index.html</link>
  <description><![CDATA[ 



<section id="unexpected-simplicity" class="level2">
<h2 class="anchored" data-anchor-id="unexpected-simplicity">Unexpected Simplicity</h2>
<p>The timeline of working through <a href="https://karpathy.ai/">Andrej Karpathy</a>’s <a href="https://karpathy.ai/zero-to-hero.html">Neural Networks: Zero to Hero</a> course presents some early and striking milestones.</p>
<p>In the first hour of the first class, <a href="https://www.youtube.com/watch?v=VMj-3S1tku0">The spelled-out intro to neural networks and backpropagation: building micrograd</a>, you’re guided through building and manually verifying a single step of backpropagation of a simple mathematical expression. It’s mostly composed of a cute <code>Value</code> class that knows how to add and multiply other <code>Value</code>s, as well as keeping track of it’s child <code>Value</code>s and local gradient.</p>
<p>You build up an expression of values, visualize it using <a href="https://graphviz.org/"><code>graphviz</code></a>, then step through each <code>Value</code> node recursively computing the derivatives of sentinel node of the expression with respect to each “backwardly” successive child expression, leveraging the chain rule.</p>
<p>To resurface an intuition for the chain rule, Karpathy pulls up the example on <a href="https://en.wikipedia.org/wiki/Chain_rule">Wikipedia</a> :</p>
<blockquote class="blockquote">
<p>As put by George F. Simmons: “If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.</p>
</blockquote>
<blockquote class="blockquote">
<p>Let <img src="https://latex.codecogs.com/png.latex?z">, <img src="https://latex.codecogs.com/png.latex?y"> and <img src="https://latex.codecogs.com/png.latex?x"> be the (variable) positions of the car, the bicycle, and the walking man, respectively. The rate of change of relative positions of the car and the bicycle is <img src="https://latex.codecogs.com/png.latex?dz/dy=2">. Similarly, <img src="https://latex.codecogs.com/png.latex?dy/dx=4">.</p>
</blockquote>
<blockquote class="blockquote">
<p>So, the rate of change of the relative positions of the car and the walking man is</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0Adz/dx%20=%20dz/dy%20*%20dy/dx%20=%202%20*%204%20=8%0A"></p>
<p>At this point you’ve reconstructed a healthy chunk of the core of <a href="https://github.com/karpathy/micrograd">micrograd</a>, an engine for small neural networks. The ultimate (at the time of writing) codebase is small: <a href="https://github.com/karpathy/micrograd/blob/c911406e5ace8742e5841a7e0df113ecb5d54685/micrograd/engine.py"><code>engine.py</code></a> that is 94 LOC, <a href="https://github.com/karpathy/micrograd/blob/c911406e5ace8742e5841a7e0df113ecb5d54685/micrograd/nn.py"><code>nn.py</code></a> that is 60 LOC.</p>
<p>Above is a (simplified) description of backpropagation and a python implementation described in 270-odd words. Glibly, one could describe the kernel of neural networks as the recursively applied chain rule through a graph.</p>
</section>
<section id="unexpected-complexity" class="level2">
<h2 class="anchored" data-anchor-id="unexpected-complexity">Unexpected Complexity</h2>
<p>Note that <a href="https://github.com/karpathy/micrograd"><code>micrograd</code></a> includes in it’s <code>README</code> description of itself as “Potentially useful for educational purposes.”</p>
<p>Karpathy also notes in the lecture video that a lot of the complexity in making neural network training work is having them run efficiently. He also notes that micrograd eschews tensors as a data structure to favor pedagogy.</p>
<p>The backpropagation algorithm itself having a recursive structure is subject to efficiency improvement through <a href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a>.</p>
<p>Hardware parallelization techniques such as SIMD that <a href="https://github.com/endorno/pytorch/blob/master/torch/lib/TH/generic/simd/simd.h">PyTorch et al leverage</a> make training a neural network reasonable relative to the time it would take for serial operations to run.</p>
<p>The LLM neural network inference library, llama.cpp, generates predictions based on context and inputs. While this functionality is tangential to the training of the neural network, it too has a massive iceberg of performance optimizations as noted by <a href="https://justine.lol/matmul/">jart</a> in a recent outline of enhancements including:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Mmap">mmap</a> support for loading weights</li>
<li>identifying constants for parameters that are never used</li>
<li>unrolling the right loops to share registers, without stepping on CPU speculation</li>
</ul>
</section>
<section id="simple-complex" class="level2">
<h2 class="anchored" data-anchor-id="simple-complex">{Simple, Complex}</h2>
<p>The complexity iceberg meme reigns supreme in the context of neural networks. Much like exposing a simple interface and hiding the complexity behind it, or <a href="https://web.stanford.edu/~ouster/cgi-bin/cs190-winter18/lecture.php?topic=modularDesign">“deep modules”</a> à la John Ousterhout, the unexpected simplicitiy of the initial concepts give way to the unexpected complexity of the implementation.</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>ai</category>
  <guid>https://conbrad.ca/notes/backprop-recursive-chain-rule/index.html</guid>
  <pubDate>Sat, 06 Apr 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Hello World</title>
  <dc:creator>Conor Brady</dc:creator>
  <link>https://conbrad.ca/notes/hello-world/index.html</link>
  <description><![CDATA[ 



<section id="this-is-a-test" class="level3">
<h3 class="anchored" data-anchor-id="this-is-a-test">This is a test</h3>
<pre><code>println("Hello World")</code></pre>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>news</category>
  <guid>https://conbrad.ca/notes/hello-world/index.html</guid>
  <pubDate>Thu, 24 Aug 2023 07:00:00 GMT</pubDate>
</item>
</channel>
</rss>
