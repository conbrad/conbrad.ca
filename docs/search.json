[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "conbrad.ca",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\n4/6/24\n\n\nUnexpected {Simplicity, Complexity}\n\n\n\n\n8/24/23\n\n\nHello World\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "notes/backprop-recursive-chain-rule/index.html",
    "href": "notes/backprop-recursive-chain-rule/index.html",
    "title": "Unexpected {Simplicity, Complexity}",
    "section": "",
    "text": "The timeline of working through Andrej Karpathy’s Neural Networks: Zero to Hero course presents some early and striking milestones.\nIn the first hour of the first class, The spelled-out intro to neural networks and backpropagation: building micrograd, you’re guided through building and manually verifying a single step of backpropagation of a simple mathematical expression. It’s mostly composed of a cute Value class that knows how to add and multiply other Values, as well as keeping track of it’s child Values and local gradient.\nYou build up an expression of values, visualize it using graphviz, then step through each Value node recursively computing the derivatives of sentinel node of the expression with respect to each “backwardly” successive child expression, leveraging the chain rule.\nTo resurface an intuition for the chain rule, Karpathy pulls up the example on Wikipedia :\n\nAs put by George F. Simmons: “If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.\n\n\nLet \\(z\\), \\(y\\) and \\(x\\) be the (variable) positions of the car, the bicycle, and the walking man, respectively. The rate of change of relative positions of the car and the bicycle is \\(dz/dy=2\\). Similarly, \\(dy/dx=4\\).\n\n\nSo, the rate of change of the relative positions of the car and the walking man is\n\n\\[\ndz/dx = dz/dy * dy/dx = 2 * 4 =8\n\\]\nAt this point you’ve reconstructed a healthy chunk of the core of micrograd, an engine for small neural networks. The ultimate (at the time of writing) codebase is small: engine.py that is 94 LOC, nn.py that is 60 LOC.\nAbove is a (simplified) description of backpropagation and a python implementation described in 270-odd words. Glibly, one could describe the kernel of neural networks as the recursively applied chain rule through a graph."
  },
  {
    "objectID": "notes/backprop-recursive-chain-rule/index.html#unexpected-simplicity-unexpected-complexity",
    "href": "notes/backprop-recursive-chain-rule/index.html#unexpected-simplicity-unexpected-complexity",
    "title": "Backpropagation: Recursively Applied Chain Rule Through a Graph",
    "section": "",
    "text": "The timeline of working through Andrej Karpathy’s Neural Networks: Zero to Hero course presents some early and striking milestones.\nIn the first hour of the first class, The spelled-out intro to neural networks and backpropagation: building micrograd, you’re guided through building and manually verifying a single step of backpropagation of a simple mathematical expression. It’s mostly composed of a cute Value class that knows how to add and multiply other Values, as well as keeping track of it’s child Values and local gradient.\nYou build up an expression of values, visualize it using graphviz, then step through each Value node recursively computing the derivatives of sentinel node of the expression with respect to each “backwardly” successive child expression, leveraging the chain rule.\nTo resurface an intuition for the chain rule, Karpathy pulls up the example on Wikipedia :\n\nAs put by George F. Simmons: “If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.\n\n\nLet \\(z\\), \\(y\\) and \\(x\\) be the (variable) positions of the car, the bicycle, and the walking man, respectively. The rate of change of relative positions of the car and the bicycle is \\(dz/dy=2\\). Similarly, \\(dy/dx=4\\).\n\n\nSo, the rate of change of the relative positions of the car and the walking man is\n\n\\[\ndz/dx = dz/dy * dy/dx = 2 * 4 =8\n\\]\nAt this point you’ve reconstructed a healthy chunk of the core of micrograd, an engine for small neural networks. The ultimate (at the time of writing) codebase is small: engine.py that is 94 LOC, nn.py that is 60 LOC.\nA (simplified) description of backpropagation and a python implementation described in 270-odd words suprised me with simplicity. So where is the complexity?"
  },
  {
    "objectID": "notes/backprop-recursive-chain-rule/index.html#unexpected-simplicity",
    "href": "notes/backprop-recursive-chain-rule/index.html#unexpected-simplicity",
    "title": "Unexpected {Simplicity, Complexity}",
    "section": "",
    "text": "The timeline of working through Andrej Karpathy’s Neural Networks: Zero to Hero course presents some early and striking milestones.\nIn the first hour of the first class, The spelled-out intro to neural networks and backpropagation: building micrograd, you’re guided through building and manually verifying a single step of backpropagation of a simple mathematical expression. It’s mostly composed of a cute Value class that knows how to add and multiply other Values, as well as keeping track of it’s child Values and local gradient.\nYou build up an expression of values, visualize it using graphviz, then step through each Value node recursively computing the derivatives of sentinel node of the expression with respect to each “backwardly” successive child expression, leveraging the chain rule.\nTo resurface an intuition for the chain rule, Karpathy pulls up the example on Wikipedia :\n\nAs put by George F. Simmons: “If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.\n\n\nLet \\(z\\), \\(y\\) and \\(x\\) be the (variable) positions of the car, the bicycle, and the walking man, respectively. The rate of change of relative positions of the car and the bicycle is \\(dz/dy=2\\). Similarly, \\(dy/dx=4\\).\n\n\nSo, the rate of change of the relative positions of the car and the walking man is\n\n\\[\ndz/dx = dz/dy * dy/dx = 2 * 4 =8\n\\]\nAt this point you’ve reconstructed a healthy chunk of the core of micrograd, an engine for small neural networks. The ultimate (at the time of writing) codebase is small: engine.py that is 94 LOC, nn.py that is 60 LOC.\nAbove is a (simplified) description of backpropagation and a python implementation described in 270-odd words. Glibly, one could describe the kernel of neural networks as the recursively applied chain rule through a graph."
  },
  {
    "objectID": "notes/backprop-recursive-chain-rule/index.html#unexpected-complexity",
    "href": "notes/backprop-recursive-chain-rule/index.html#unexpected-complexity",
    "title": "Unexpected {Simplicity, Complexity}",
    "section": "Unexpected Complexity",
    "text": "Unexpected Complexity\nNote that micrograd includes in it’s README description of itself as “Potentially useful for educational purposes.”\nKarpathy also notes in the lecture video that a lot of the complexity in making neural network training work is having them run efficiently. He also notes that micrograd eschews tensors as a data structure to favor pedagogy.\nThe backpropagation algorithm itself having a recursive structure is subject to efficiency improvement through dynamic programming.\nHardware parallelization techniques such as SIMD that PyTorch et al leverage make training a neural network reasonable relative to the time it would take for serial operations to run.\nThe LLM neural network inference library, llama.cpp, generates predictions based on context and inputs. While this functionality is tangential to the training of the neural network, it too has a massive iceberg of performance optimizations as noted by jart in a recent outline of enhancements including:\n\nmmap support for loading weights\nidentifying constants for parameters that are never used\nunrolling the right loops to share registers, without stepping on CPU speculation"
  },
  {
    "objectID": "notes/backprop-recursive-chain-rule/index.html#simple-complex",
    "href": "notes/backprop-recursive-chain-rule/index.html#simple-complex",
    "title": "Unexpected {Simplicity, Complexity}",
    "section": "{Simple, Complex}",
    "text": "{Simple, Complex}\nThe complexity iceberg meme reigns supreme in the context of neural networks. Much like exposing a simple interface and hiding the complexity behind it, or “deep modules” à la John Ousterhout, the unexpected simplicitiy of the initial concepts give way to the unexpected complexity of the implementation."
  },
  {
    "objectID": "notes/hello-world/index.html",
    "href": "notes/hello-world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "This is a test\nprintln(\"Hello World\")\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Conor Brady is a software engineer currently working on predicting wildfire behaviour and mesh networks."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\n\nBC Wildfire - Predictive Services Unit\nFull Stack Developer | March 2021 - present\n\n\nBump\nSenior Software Engineer | December 2022 - October 2023\n\n\nSpare Labs\nSoftware Engineer | August 2019 - July 2020\n\n\nRightMesh\nSoftware Engineer | September 2018 - August 2019\n\n\nScout24\nSoftware Engineer | September 2017 - August 2018\n\n\nVisier\nSoftware Developer | August 2016 - April 2017"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nSimon Fraser University\nBSc in Computing Science | Sept 2012 - June 2016\n\n\nUniversity of British Columbia\nResearch Assistant - ECE | July 2015 - July 2016"
  }
]