---
title: "single neuron"
author: "Conor Brady"
date: "2024-04-06"
categories: [ai]
---


## Notes
- bias of neuron is constrained via a non-linear function like `tanh`
- topo sort to backpropagate gradients through graph
- guard to promote prim number to `Value` as well as using `r` variants of `mul` and `add` to make things easier 
- redefine divison as `self * other**-1`
- redefine subtraction as addition of a negation
- negation is multiplying by -1
- tensors are n-dimensional arrays of scalars
- loss function is single value measuring performance
    - use mse or abs, just need to discard the possible negative sign

- forward pass: for each layer in mlp, for each neuron in layer, for each parameter in neuron, nudge gradient by small amount
- backward pass: recompute gradients of topo sort
- continue above until loss is minimal