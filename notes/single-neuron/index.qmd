---
title: "single neuron"
author: "Conor Brady"
date: "2024-04-06"
categories: [ai]
---


## Notes
- bias of neuron is constrained via a non-linear function like `tanh`
- topo sort to backpropagate gradients through graph
- guard to promote prim number to `Value` as well as using `r` variants of `mul` and `add` to make things easier 
- redefine divison as `self * other**-1`
- redefine subtraction as addition of a negation
- negation is multiplying by -1
- tensors are n-dimensional arrays of scalars
- loss function is single value measuring performance
    - use mse or abs, just need to discard the possible negative sign

- forward pass: for each layer in mlp, for each neuron in layer, for each parameter in neuron, nudge gradient by small amount
- backward pass: recompute gradients of topo sort
- continue above until loss is minimal

### Neural Network
![Fig 1. Neural Network](neural_net2.jpeg)

- composes layers of neurons, layers compose neurons, and tie outputs from one layer to the next layer
- finally maps outputs from 2nd last layer to output layer with single neuron

### Neuron
![Fig 2. Neuron](neuron_model.jpeg)

- sum up weights and bias for each neuron
- apply activation function to sum of weights, which clamps extremes
    - this is why `tan h` is used

